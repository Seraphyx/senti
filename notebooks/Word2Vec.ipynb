{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, division\n",
    "import io\n",
    "import bz2\n",
    "import logging\n",
    "from os import path\n",
    "import os\n",
    "import random\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import plac\n",
    "try:\n",
    "    import ujson as json\n",
    "except ImportError:\n",
    "    import json\n",
    "from gensim.models import Word2Vec\n",
    "from preshed.counter import PreshCounter\n",
    "from spacy.strings import hash_string\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, directory, min_freq=10):\n",
    "        self.directory = directory\n",
    "        self.counts = PreshCounter()\n",
    "        self.strings = {}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def count_doc(self, words):\n",
    "        # Get counts for this document\n",
    "        doc_counts = PreshCounter()\n",
    "        doc_strings = {}\n",
    "        for word in words:\n",
    "            key = hash_string(word)\n",
    "            doc_counts.inc(key, 1)\n",
    "            doc_strings[key] = word\n",
    "        n = 0\n",
    "        for key, count in doc_counts:\n",
    "            self.counts.inc(key, count)\n",
    "            # TODO: Why doesn't inc return this? =/\n",
    "            corpus_count = self.counts[key]\n",
    "            # Remember the string when we exceed min count\n",
    "            if corpus_count >= self.min_freq and (corpus_count - count) < self.min_freq:\n",
    "                 self.strings[key] = doc_strings[key]\n",
    "            n += count\n",
    "        return n\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text_loc in iter_dir(self.directory):\n",
    "            with io.open(text_loc, 'r', encoding='utf8') as file_:\n",
    "                sent_strs = list(file_)\n",
    "                random.shuffle(sent_strs)\n",
    "                for sent_str in sent_strs:\n",
    "                    yield sent_str.split()\n",
    "\n",
    "\n",
    "def iter_dir(loc):\n",
    "    for fn in os.listdir(loc):\n",
    "        if path.isdir(path.join(loc, fn)):\n",
    "            for sub in os.listdir(path.join(loc, fn)):\n",
    "                yield path.join(loc, fn, sub)\n",
    "        else:\n",
    "            yield path.join(loc, fn)\n",
    "\n",
    "@plac.annotations(\n",
    "    in_dir=(\"Location of input directory\"),\n",
    "    out_loc=(\"Location of output file\"),\n",
    "    n_workers=(\"Number of workers\", \"option\", \"n\", int),\n",
    "    size=(\"Dimension of the word vectors\", \"option\", \"d\", int),\n",
    "    window=(\"Context window size\", \"option\", \"w\", int),\n",
    "    min_count=(\"Min count\", \"option\", \"m\", int),\n",
    "    negative=(\"Number of negative samples\", \"option\", \"g\", int),\n",
    "    nr_iter=(\"Number of iterations\", \"option\", \"i\", int),\n",
    ")\n",
    "def main(in_dir, out_loc, negative=5, n_workers=4, window=5, size=128, min_count=10, nr_iter=2):\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Word2Vec(\n",
    "        size=size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=n_workers,\n",
    "        sample=1e-5,\n",
    "        negative=negative\n",
    "    )\n",
    "    corpus = Corpus(in_dir)\n",
    "    total_words = 0\n",
    "    total_sents = 0\n",
    "    for text_no, text_loc in enumerate(iter_dir(corpus.directory)):\n",
    "        print(\"\\tReading file: %s\" % text_loc)\n",
    "        with io.open(text_loc, 'r', encoding='utf8') as file_:\n",
    "            text = file_.read()\n",
    "        total_sents += text.count('\\n')\n",
    "        total_words += corpus.count_doc(text.split())  \n",
    "        logger.info(\"PROGRESS: at batch #%i, processed %i words, keeping %i word types\",\n",
    "                    text_no, total_words, len(corpus.strings))\n",
    "    model.corpus_count = total_sents\n",
    "    model.raw_vocab = defaultdict(int)\n",
    "    for key, string in corpus.strings.items():\n",
    "        model.raw_vocab[string] = corpus.counts[key]\n",
    "    model.scale_vocab()\n",
    "    model.finalize_vocab()\n",
    "    model.iter = nr_iter\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=nr_iter)\n",
    "\n",
    "    model.save(out_loc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_count**: One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them. A reasonable value for min_count is between 0-100, depending on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-25 05:25:40,838 : INFO : PROGRESS: at batch #0, processed 438 words, keeping 7 word types\n",
      "2017-11-25 05:25:40,839 : INFO : PROGRESS: at batch #1, processed 712 words, keeping 9 word types\n",
      "2017-11-25 05:25:40,842 : INFO : PROGRESS: at batch #2, processed 963 words, keeping 13 word types\n",
      "2017-11-25 05:25:40,843 : INFO : Loading a fresh vocabulary\n",
      "2017-11-25 05:25:40,843 : INFO : min_count=5 retains 13 unique words (100% of original 13, drops 0)\n",
      "2017-11-25 05:25:40,844 : INFO : min_count=5 leaves 294 word corpus (100% of original 294, drops 0)\n",
      "2017-11-25 05:25:40,844 : INFO : deleting the raw counts dictionary of 13 items\n",
      "2017-11-25 05:25:40,845 : INFO : sample=1e-05 downsamples 13 most-common words\n",
      "2017-11-25 05:25:40,845 : INFO : downsampling leaves estimated 3 word corpus (1.1% of prior 294)\n",
      "2017-11-25 05:25:40,846 : INFO : estimated required memory for 13 words and 128 dimensions: 19812 bytes\n",
      "2017-11-25 05:25:40,846 : INFO : resetting layer weights\n",
      "2017-11-25 05:25:40,847 : INFO : training model with 8 workers on 13 vocabulary and 128 features, using sg=0 hs=0 sample=1e-05 negative=5 window=5\n",
      "2017-11-25 05:25:40,859 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-11-25 05:25:40,860 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-11-25 05:25:40,860 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-11-25 05:25:40,861 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-11-25 05:25:40,862 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-25 05:25:40,862 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-25 05:25:40,863 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-25 05:25:40,864 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-25 05:25:40,865 : INFO : training on 1926 raw words (6 effective words) took 0.0s, 535 effective words/s\n",
      "2017-11-25 05:25:40,866 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-11-25 05:25:40,867 : INFO : saving Word2Vec object under ../embeddings/test/test_word2vec, separately None\n",
      "2017-11-25 05:25:40,868 : INFO : not storing attribute syn0norm\n",
      "2017-11-25 05:25:40,869 : INFO : not storing attribute cum_table\n",
      "2017-11-25 05:25:40,876 : INFO : saved ../embeddings/test/test_word2vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tReading file: ../data/text/test/test_1.txt\n",
      "\tReading file: ../data/text/test/test_2.txt\n",
      "\tReading file: ../data/text/test/test_3.txt\n"
     ]
    }
   ],
   "source": [
    "main(in_dir='../data/text/test', \n",
    "     out_loc='../embeddings/test/test_word2vec', \n",
    "     negative=5, \n",
    "     n_workers=8, \n",
    "     window=5, \n",
    "     size=128, \n",
    "     min_count=5, \n",
    "     nr_iter=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-25 05:29:16,556 : INFO : loading Word2Vec object from ../embeddings/test/test_word2vec\n",
      "2017-11-25 05:29:16,561 : INFO : loading wv recursively from ../embeddings/test/test_word2vec.wv.* with mmap=None\n",
      "2017-11-25 05:29:16,562 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-25 05:29:16,563 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-25 05:29:16,566 : INFO : loaded ../embeddings/test/test_word2vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 13 vecabs\n",
      "{'I': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467da20>,\n",
      " 'It': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467dba8>,\n",
      " 'a': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467dac8>,\n",
      " 'and': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467da90>,\n",
      " 'game': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467d7b8>,\n",
      " 'good': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467dc18>,\n",
      " 'is': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467d908>,\n",
      " 'it': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467d898>,\n",
      " 'love': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467dcf8>,\n",
      " 'so': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467db38>,\n",
      " 'the': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467d978>,\n",
      " 'this': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467dc88>,\n",
      " 'to': <gensim.models.keyedvectors.Vocab object at 0x7f9d9467d9e8>}\n",
      "\n",
      "Single Vocab\n",
      "Vocab(count:41, index:1, sample_int:36677838)\n",
      "\n",
      "Keras Layer\n",
      "<keras.layers.embeddings.Embedding object at 0x7f9d9460e470>\n",
      "\n",
      "Vector\n",
      "[  2.83664861e-03  -1.62266695e-03  -2.58724694e-03   1.62974000e-04\n",
      "  -1.84019402e-04  -1.72825169e-03   1.73147116e-03  -1.01258710e-03\n",
      "   2.36792490e-03   8.45635368e-05  -2.79628555e-03  -1.17873494e-03\n",
      "  -6.76008931e-04  -1.61486946e-03   1.89239203e-04   3.00700334e-03\n",
      "  -9.18436155e-04  -2.64288485e-03  -1.43939140e-03  -3.52766132e-03\n",
      "  -1.71845080e-04   2.20614253e-03   3.51519510e-03  -2.50300020e-03\n",
      "  -8.68741015e-04  -1.30748248e-03  -1.72223616e-03   4.09177272e-04\n",
      "  -2.73708813e-03  -1.47199375e-03  -3.28184688e-04   2.87868711e-03\n",
      "  -6.94902483e-05  -3.74248228e-03   3.01109132e-04   1.13139814e-03\n",
      "   1.35354954e-03   1.62380142e-03  -1.37443643e-03   1.82755222e-03\n",
      "   1.71137182e-03  -3.13130877e-04   2.07264442e-03   1.82719703e-03\n",
      "   1.52888021e-03   2.34971242e-03   8.75105790e-04  -1.04098546e-03\n",
      "   2.23587290e-03  -3.82764265e-03   8.29887751e-04  -2.39630649e-03\n",
      "   2.28118803e-03  -3.15185403e-03  -1.20379415e-03  -2.77294358e-03\n",
      "  -2.80116964e-03  -2.98018940e-03   1.55855878e-03   1.42678351e-03\n",
      "  -4.96380322e-04  -3.12781939e-03   6.78122335e-04  -8.23479437e-04\n",
      "   3.50677152e-03  -2.57697771e-03   9.54403018e-04  -3.01392144e-03\n",
      "  -7.29697582e-04   2.45393626e-03  -3.18438490e-03   2.03840435e-03\n",
      "  -1.95398997e-03  -3.77026107e-03   1.06191065e-03  -3.27329803e-03\n",
      "  -3.76388198e-03  -3.27698165e-03  -1.00674026e-03  -3.64554371e-03\n",
      "   2.05798517e-03   3.57038178e-03  -3.77691817e-04  -1.97767164e-03\n",
      "   2.93079345e-03   3.44449945e-04   3.82437045e-03   1.90735422e-03\n",
      "  -1.13901508e-03  -3.63424630e-03  -9.13825992e-04  -1.75720092e-03\n",
      "   4.81678289e-04  -2.63865135e-04   1.37154257e-03  -3.14762234e-03\n",
      "   3.49238329e-03  -3.66325025e-03   1.35312020e-03  -2.83867656e-03\n",
      "   3.75274965e-03  -1.92808965e-03   1.11598382e-03  -3.76491528e-03\n",
      "  -1.55573501e-03   3.86813120e-03  -1.27667200e-03  -1.04873569e-03\n",
      "   2.22473592e-03  -2.89765373e-03  -1.22608780e-03  -2.08473438e-03\n",
      "   5.87921531e-04  -2.41024769e-03   3.27147567e-03  -8.65026552e-04\n",
      "   1.22810446e-03   2.69253599e-03   2.66902009e-03  -1.72818254e-03\n",
      "   2.84645311e-03   2.21789675e-03   2.77439668e-03   8.19030276e-04\n",
      "  -1.93459541e-03  -1.10331993e-03  -2.14114203e-03  -1.87963760e-03]\n"
     ]
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec.load('../embeddings/test/test_word2vec')\n",
    "print(\"\\nThere are %i vecabs\" % len(new_model.wv.vocab))\n",
    "pprint(new_model.wv.vocab)\n",
    "\n",
    "print(\"\\nSingle Vocab\")\n",
    "print(new_model.wv.vocab['game'])\n",
    "\n",
    "print(\"\\nKeras Layer\")\n",
    "print(new_model.wv.get_keras_embedding())\n",
    "\n",
    "print(\"\\nVector\")\n",
    "print(new_model.wv.word_vec(word='game', use_norm=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online training / Resuming training\n",
    "Advanced users can load a model and continue training it with more sentences:\n",
    "\n",
    "`model = gensim.models.Word2Vec.load('/tmp/mymodel')\n",
    "model.train(more_sentences)`\n",
    "\n",
    "You may need to tweak the total_words parameter to train(), depending on what learning rate decay you want to simulate.\n",
    "\n",
    "Note that it’s not possible to resume training with models generated by the C tool, load_word2vec_format(). You can still use them for querying/similarity, but information vital for training (the vocab tree) is missing there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec.load('/tmp/mymodel')\n",
    "# model.train(more_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "os.environ['KERAS_BACKEND']='tensorflow'\n",
    "print(os.environ['KERAS_BACKEND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/notebooks\n",
      "0\n",
      "b'total 48\\n-rwxr-xr-x 1 root root 20571 Nov 12 14:54 Hierarchical Attention Network.ipynb\\n-rwxr-xr-x 1 root root 10483 Nov 24 07:16 spacy-tutorial.ipynb\\n-rwxr-xr-x 1 root root 11720 Nov 25 05:23 Word2Vec.ipynb\\n'\n"
     ]
    }
   ],
   "source": [
    "from subprocess import call, check_output\n",
    "print(os.getcwd())\n",
    "print(call([\"pwd\", \"|\"]))\n",
    "print(check_output(['ls','-l']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
